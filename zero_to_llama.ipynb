{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "from sentencepiece import SentencePieceProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much memory is required for inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1 GB'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = 7000000000 # 7B\n",
    "quant = 2 # float16\n",
    "\n",
    "def get_mem_requirements(params, quant):\n",
    "    \"\"\"\n",
    "    Memory requirements for inference:\n",
    "    4bytes per parameter, 32/quant bits per parameter (e.g. 32/16 for float16), 1.2 additional memory overhead factor\n",
    "    For training ~ (params * 4bytes) * 4\n",
    "    \"\"\"\n",
    "    return str((((params * 4)/(32/quant)) * 1.2) * 1e-9) + \" GB\"\n",
    "\n",
    "get_mem_requirements(params, quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = 32000 # -1\n",
    "    multiple_of: int = 256\n",
    "    ffn_dim_multiplier: Optional[int] = None\n",
    "    norm_eps: float = 1e-5\n",
    "    batch_size: int = 1\n",
    "    seq_len: int = 2048\n",
    "    device: str = DEVICE\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precomputed_freqs(dim: int, seqlen: int, device: str, theta: float = 10000.0):\n",
    "    assert dim % 2 == 0, \"Dim must be even\"\n",
    "\n",
    "    I = torch.arange(0, dim, 2, dtype=torch.float32, device=device)\n",
    "    theta = 1.0 / (theta ** (I / dim))\n",
    "    m = torch.arange(seqlen, device=device).float()\n",
    "    freqs = torch.outer(m, theta)\n",
    "    freqs = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs\n",
    "\n",
    "\n",
    "def rope(x: torch.Tensor, freqs: torch.Tensor):\n",
    "    x_cmplx = torch.view_as_complex(x.reshape(*x.shape[:-1], -1, 2))\n",
    "    freqs = freqs.unsqueeze(0).unsqueeze(2)\n",
    "    x_rotated = x_cmplx * freqs\n",
    "    x_rotated = torch.view_as_real(x_rotated).reshape(*x.shape)\n",
    "    return x_rotated.type_as(x).to(x.device)\n",
    "\n",
    "\n",
    "def rep_tensor(x: torch.Tensor, n_rep: int):\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    else:\n",
    "        return torch.repeat_interleave(x, dim=2, repeats=n_rep)\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float=1e-6):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        return (x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)) * self.w\n",
    "        \n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, args: Args):\n",
    "        super().__init__()\n",
    "        self.dim = args.dim\n",
    "        self.n_q_heads = args.n_heads\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        assert self.n_q_heads % self.n_kv_heads == 0, \"n_q_heads must be divisible by n_kv_heads\"\n",
    "        self.n_rep = self.n_q_heads // self.n_kv_heads\n",
    "        self.h_size = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.LazyLinear(self.n_q_heads * self.h_size, bias=False)\n",
    "        self.wk = nn.LazyLinear(self.n_kv_heads * self.h_size, bias=False)\n",
    "        self.wv = nn.LazyLinear(self.n_kv_heads * self.h_size, bias=False)\n",
    "        self.wo = nn.LazyLinear(args.dim, bias=False)\n",
    "\n",
    "        self.k_cache = torch.empty((args.batch_size, args.seq_len, self.n_kv_heads, self.h_size), device=DEVICE)\n",
    "        self.v_cache = torch.empty((args.batch_size, args.seq_len, self.n_kv_heads, self.h_size), device=DEVICE)\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs: torch.Tensor, mask: Optional[torch.Tensor]=None):\n",
    "        B, T, D = x.shape\n",
    "        assert D == self.dim, f\"x.shape[2] --> dim mismatch: {D} != {self.dim}\"\n",
    "        assert D == self.n_q_heads * self.h_size, f\"x.shape[2] --> dim mismatch: {D} != {self.n_q_heads} * {self.h_size}\"\n",
    "\n",
    "        q = self.wq(x) # B, T, n_q_head * hsize\n",
    "        _k = self.wk(x) # B, T, n_kv_heads * hsize\n",
    "        _v = self.wv(x) # B, T, n_kv_heads * hsize\n",
    "\n",
    "        q = q.view(B, T, self.n_q_heads, self.h_size)\n",
    "        _k = _k.view(B, T, self.n_kv_heads, self.h_size)\n",
    "        _v = _v.view(B, T, self.n_kv_heads, self.h_size)\n",
    "        \n",
    "        # apply RoPE, same shape\n",
    "        q = rope(q, freqs)\n",
    "        _k = rope(_k, freqs)\n",
    "\n",
    "        # kv - cache\n",
    "        self.k_cache[:B, start_pos :start_pos + T] = _k \n",
    "        self.v_cache[:B, start_pos :start_pos + T] = _v\n",
    "\n",
    "        # B, start_pos + T, n_kv_heads, hsize\n",
    "        k = self.v_cache[:B, :start_pos + T]\n",
    "        # B, start_pos + T, n_kv_heads, hsize\n",
    "        v = self.v_cache[:B, :start_pos + T]\n",
    "\n",
    "        # B, start_pos + T, n_kv_heads, hsize --> B, start_pos + T, n_rep * n_kv_heads, hsize\n",
    "        # n_reps * n_kv_heads = n_q_heads\n",
    "        k, v = rep_tensor(k, self.n_rep), rep_tensor(v, self.n_rep)\n",
    "        \n",
    "        q = q.transpose(1, 2) # B, n_q_heads, T, hsize\n",
    "        k = k.transpose(1, 2) # B, n_q_heads, start_pos + T, hsize\n",
    "        v = v.transpose(1, 2) # B, n_q_heads, start_pos + T, hsize\n",
    "\n",
    "        # B, n_q_heads, [(T , hsize) @ (hsize, start_pos + T)] --> B, n_q_heads, T, start_pos + T\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * math.sqrt(self.h_size)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn += mask\n",
    "        \n",
    "        # B, n_q_heads, T, start_pos + T\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # B, n_q_heads [(T, start_pos + T) @ (start_pos + T, hsize)]  --> B, n_q_heads, T, hsize\n",
    "        attn = torch.matmul(attn, v)\n",
    "\n",
    "        # B, nheads, T, hsize --> B, T, nheads * hsize\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, T, self.n_q_heads * self.h_size)\n",
    "\n",
    "        # B, T, D\n",
    "        attn = self.wo(attn)\n",
    "\n",
    "        return attn\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim: int, multiple_of: int, ffn_dim_multiplier: Optional[int]=None):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = 4 * dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        \n",
    "        # Round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        # print(f\"hidden_dim: {hidden_dim}\")\n",
    "\n",
    "        self.w1 = nn.LazyLinear(hidden_dim, bias=False)\n",
    "        self.w2 = nn.LazyLinear(dim, bias=False)\n",
    "        self.w3 = nn.LazyLinear(hidden_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, args: Args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = SelfAttention(args)\n",
    "        self.attn_norm = RMSNorm(args.dim)\n",
    "        self.ffn = FFN(args.dim, args.multiple_of)\n",
    "        self.ffn_norm = RMSNorm(args.dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs: torch.Tensor, mask: Optional[torch.Tensor]=None):\n",
    "        x_ = x + self.attention(self.attn_norm(x), start_pos, mask, freqs)\n",
    "        out = x_ + self.ffn(self.ffn_norm(x_))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, args: Args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.n_layers = args.n_layers\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for layer in range(self.n_layers):\n",
    "            self.layers.append(TransformerBlock(args))\n",
    "        \n",
    "        self.norm = RMSNorm(args.dim, args.norm_eps)\n",
    "        self.out = nn.LazyLinear(self.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs = precomputed_freqs(args.dim // args.n_heads, args.seq_len * 2, args.device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int):\n",
    "        B, T = x.shape\n",
    "        x_embd = self.tok_embeddings(x) # B, T --> B, T, D\n",
    "\n",
    "        #rope freqs\n",
    "        freqs = self.freqs[start_pos:start_pos + T]\n",
    "        mask = None\n",
    "        if T > 1:\n",
    "            mask = torch.full((T, T), float(\"-inf\"), device=x.device)\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            mask = torch.hstack([torch.zeros((T, start_pos), device=x.device),mask]).type_as(x_embd)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x_embd = layer(x_embd, start_pos, mask, freqs)\n",
    "\n",
    "        x_embd = self.norm(x_embd)\n",
    "\n",
    "        return self.out(x_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers for loading HF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rot_emb(state_dict):\n",
    "    keys = list(state_dict.keys())\n",
    "    for k in keys:\n",
    "        if \"inv_freq\" in k:\n",
    "            del state_dict[f\"{k}\"]\n",
    "            # print(k)\n",
    "    return state_dict\n",
    "\n",
    "def transform_key(key):\n",
    "    patterns = [\n",
    "        (\"input_layernorm.weight\", \"ffn_norm.w\"),\n",
    "        (\"post_attention_layernorm.weight\", \"attn_norm.w\"),\n",
    "        (\"self_attn.k_proj\", \"attention.wk\"),\n",
    "        (\"self_attn.o_proj\", \"attention.wo\"),\n",
    "        (\"self_attn.q_proj\", \"attention.wq\"),\n",
    "        (\"self_attn.v_proj\", \"attention.wv\"),\n",
    "        (\"mlp.down_proj\", \"ffn.w2\"),\n",
    "        (\"mlp.gate_proj\", \"ffn.w3\"),\n",
    "        (\"mlp.up_proj\", \"ffn.w1\"),\n",
    "        (\"norm.weight\", \"norm.w\"),\n",
    "        (\"lm_head\", \"out\"),\n",
    "        (\"embed_tokens\", \"tok_embeddings\")\n",
    "    ]\n",
    "    \n",
    "    for old_pattern, new_pattern in patterns:\n",
    "        if old_pattern in key:\n",
    "            return key.replace(old_pattern, new_pattern)\n",
    "    \n",
    "    return key\n",
    "\n",
    "def rename_state_dict_keys(state_dict):\n",
    "     keys = list(state_dict.keys())\n",
    "     for key in keys:\n",
    "        new_key = transform_key(key)\n",
    "        if \"model.\" in new_key:\n",
    "            new_key = new_key.replace(\"model.\", \"\")\n",
    "        # print(f\"New: {new_k}, Old: {k}\")\n",
    "        state_dict[new_key] = state_dict.pop(key)\n",
    "     return state_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA class for loading model and tokenizer + generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLAMA:\n",
    "    def __init__(self, model: Transformer, tokenizer: SentencePieceProcessor, args: Args):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = args\n",
    "\n",
    "    @staticmethod\n",
    "    def make(checkpoints_dir: str, tokenizer_path: str, half: bool, from_hf: bool):\n",
    "        print(\"Loading tokenizer........\\n\")\n",
    "        tokenizer = SentencePieceProcessor()\n",
    "        tokenizer.load(tokenizer_path)\n",
    "        print(\"Tokenizer loaded\\n\")\n",
    "        \n",
    "        prev_time = time.time()\n",
    "\n",
    "        if from_hf:\n",
    "            checkpoints = sorted(Path(checkpoints_dir).glob(\"*.safetensors\"))\n",
    "            assert len(checkpoints) > 0, f\"no checkpoint files found in {checkpoints_dir}\"\n",
    "            ckpt_1, ckpt_2 = checkpoints\n",
    "            \n",
    "            print(f'Loading checkpoints \"{ckpt_1}\" & \"{ckpt_2}\"........\\n')\n",
    "            state_dict1 = load_file(ckpt_1)\n",
    "            state_dict2 = load_file(ckpt_2)\n",
    "            state_dict = {**state_dict1, **state_dict2}\n",
    "            \n",
    "            state_dict = remove_rot_emb(state_dict)\n",
    "            print(\"Removed rotary embedding freqs\\n\")\n",
    "            state_dict = rename_state_dict_keys(state_dict)\n",
    "            print(\"Mapped key names\\n\")\n",
    "\n",
    "        else:\n",
    "            checkpoints = sorted(Path(checkpoints_dir).glob(\"*.pth\"))\n",
    "            assert len(checkpoints) > 0, f\"no checkpoint files found in {checkpoints_dir}\"\n",
    "            checkpoint = checkpoints[0]\n",
    "            \n",
    "            print(f'Loading checkpoint \"{checkpoint}\"........\\n')\n",
    "            state_dict = torch.load(checkpoint)\n",
    "        \n",
    "        print(\"Total keys: \", len(list(state_dict.keys())))\n",
    "        \n",
    "        if half:\n",
    "            for key in state_dict:\n",
    "                if isinstance(state_dict[key], torch.FloatTensor):\n",
    "                    state_dict[key] = state_dict[key].half()\n",
    "        \n",
    "            model = Transformer(args).half().to(args.device)\n",
    "        else:\n",
    "            model = Transformer(args).to(args.device)\n",
    "\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        print(f\"\\nModel loaded, took {time.time() - prev_time:.2f} seconds\")\n",
    "        del state_dict\n",
    "        return LLAMA(model, tokenizer, args)\n",
    "\n",
    "    def generate_text(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer........\n",
      "\n",
      "Tokenizer loaded\n",
      "\n",
      "Loading checkpoint \"llama2_weights.pth\"........\n",
      "\n",
      "Total keys:  291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded, took 30.01 seconds\n"
     ]
    }
   ],
   "source": [
    "# llama = LLAMA.make(\n",
    "#         checkpoints_dir=\"Llama-2-7b-chat-hf/\",\n",
    "#         tokenizer_path=\"Llama-2-7b-chat-hf/tokenizer.model\",\n",
    "#         half=True,\n",
    "#         from_hf=True)\n",
    "\n",
    "llama = LLAMA.make(\n",
    "        checkpoints_dir=\".\",\n",
    "        tokenizer_path=\"Llama-2-7b-chat-hf/tokenizer.model\",\n",
    "        half=True,\n",
    "        from_hf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11008, 4096])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.model.layers[0].ffn.w1.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the HF model weights as .pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llama.model\n",
    "model = model.cpu()\n",
    "torch.save(model.state_dict(), 'llama2_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
